In \ref{graffiti} we desribe a system called Graffiti that
allows users to create personalized social applications
that are interoperable with each other.
The system makes it possible to build a wide range of social applications---from
Facebook to Messenger to Wikipedia---using only client side code
and with limited friction to making new applications. At the same
time, these applications interoperate.

That paper investigated developed the system from a set of design principles
that focused on the needs of individual users and communities:
their desire for personalization, the susceptibility of ``lock-in''
to applications, features, and protocols, issues around context collapse.
If it were an economics paper it would be classified as ``microeconomics.''
This paper attempts to tell the other side of the story about what might happen
if such a system were released at scale.
What usage patterns would emerge?
Would those patterns be ``better'' in any sense?
What harms might the system mitigate and what new harms might become possible?

For example, one of the major challenges of Graffiti is its lack of any central moderator.
It is different even than certain ``federated'' systems, like ActivityPub, where there
each ``server'' has its own moderator. In Graffiti, users can freely opt-in and out of different
moderation services because they do not have a.

Additionally, the choice of what is ``mine'' is reduced in the space. Users own their own words
but do not control the words of other people.

One challenge is that these societal-level effects are not logistically possible
to evaluate with a controlled user study or, to our knowledge, any existing methodology
outside of building a fully simulated world~\cite{}, of which the reality is still debated.
Moreover, it would not be morally responsible to go into such a massive-scale experiment
given the harms that social media can give rise to
at scale, which some claim can come from the design of a particular system itself:
misinformation, bullying and abuse, scams and fraud, networks of CSAM, etc.

This work aims to predict what the system might look like, to the best of our knowledge,
what the system might look like before attempting to do any large-scale release in an effort
as an attempt to not ``move fast and break things.'' While we conclude that the system
is not necessarily bad, we do point towards places that will need active attention during
a more wide scale release that warrant care.

That paper focuses on "micro" features facing inividual people
and communities. This paper focuses on the possible macroscopic
consequences of such a system deployed at scale, attempting to
pre-empt possible harms and abuses, set up infrastructure to
prevent ``enshittification,'' and also more positively optimistic
visions of what a world would look like.



% Therefore, this work attempts to do what it can to map out the space
% of what may come, and to identify what guardrails should be put in
% place without actually putting the system in the hands of millions.
% The paper will draw from works of philosophy, economics, political science,
% as well as analogies to existing systems.

We go over som

\section{Moderation}

Many works on moderation make the assumption that moderation is something
to be done by a centralized authority, and has focused on what those authorities
should do better. For example, hiring more human moderators, requiring that
[CITE guillepse]. This work could be done by external

We have made the assumption that the platforms have to be the custodians.
They may offer some levers down to users---community notes---it is up to
the platforms to set those rules.

However little work investigates the possibility of users choosing their \emph{own}
moderation. With the rise of BlueSky it is becoming possible.
Amy Zhang paper - do people want personalized moderation.
Even then the analogy is not exact.
A person may become confused with a particular configuration panel.
But not confused with a

That eric gilbert paper - moderation on BlueSky.

What might a world live in where people can choose their own moderation system?
One of the realizations is that, in a digital space, it is possible to follow different
rules simultaneously. In the physical world, for example,
a if a person poke someone else, that person perceives themselves as poked.
In Graffiti, it is possible to create a ``hit'' object where someone can hit another person,
while that other person can simply filter it out. Without the requirement for
symmetry, the landscape is very different.

What do we want to force upon other people?
Breaks any chance at paternalism---the ability for one group of people to choose what others say.

That being said, there is still the possibility of real world consequences for actions in the system.
A person may host CSAM, they may be hard-pressed to find a content provider willing to host it.
If they go great lengths to hide it technically, foresnics may be used to eventually track them down.

Is it a trolley problem?

Moreover there is evidence that platform moderation will not stop extrists.
They will find other places to do their evil so whether something can happen ``on Graffiti''
or not is not relevant to whether it can happen ``on the web.''
https://press.princeton.edu/books/hardcover/9780691258522/safe-havens-for-hate
https://www.techpolicy.press/platform-convergence-and-the-limits-of-technical-solutions-to-counter-online-hate/

\section{Universally Repugnant Content}

One aspect of Graffiti is that content is not \emph{actually} deleted,
it is just hidden.
Social norms are likely to develop where people,
but what about where cases where people would like to limit what
\emph{other} people can see?
This might be personal information, e.g. "I don't want other people to be talking about me."
or might be universally repugnant content, e.g. CSAM. Some people include terrorism although
that is a politically loaded term.

``Accidental platforming''

One aspect of Grafiti is that an

In the current implementation of Graffiti---there can be multiple---
it is possible for CSAM to be moderated away. However, we are concurrently
investigating end-to-end-encrypted services.
Much like WhatsApp or Signal, there is no way to detect.
Some apps may put client-side filters to detect CSAM pre-flight,
but there would be nothing stopping users from using other apps without those things.

\section{Echo Chambers}

Homophily is an inevitable part of social communication,
just as it is a part of real world social life.
However, some social media sites have been shown to exacerbate [CITE]
natural homophilies to create extreme ``echo chambers'' or ``filter bubbles''.

Graffiti can create these but we argue that it is more likely
to have a mediating effect compared


https://psycnet.apa.org/record/2021-32340-001?doi=1
One of the main manipulation techniques of cults is social isolation and estrangement from loved ones (Cuevas & Canto, 2006; Rodríguez-Carballeira et al., 2015). Moreover, the family is typically the main source of support for recovery, given that relatives are frequently responsible for initiating requests for pro- fessional help and advice (Dubrow-Marshall et al., 2017; Rousselet et al., 2017; Singer, 1986). Families also witness radical changes in their loved one’s behavior, personality, and customs, which is increasingly perceived as more distant, apathetic, and conflictual (Langone, 1985; Singer, 1986; Ward, 2002). In some cases, parents of young cultists report feelings of frustration and helplessness, sentiments that manifest as feelings of sadness, loss, and guilt, leading to depression or physical problems (Addis et al., 1984; Bloch & Shor, 1989).

https://www.jstor.org/stable/352224?casa_token=8eqeMbRWBoEAAAAA%3AeV3r26kofZr4-bZ8vjN47gVec_RGNNlSB9IJWtEqSmprINMU-LLK-hMYsIkkRlERy8ij7Kp6GJUhxUCOE_jck1ULb1Odbmp4noiOYCF2xgGDDtKWO2U2
corrolationbetween measures of family affinity and choices by cult devotees to leave or stay.

Cults and families:
By their very nature, cults cannot afford to have individ- uality or independently functioning families (Deikman, 1994, pp. 50–69). To this end, individual and family boundaries break down as the result of several factors. These factors include intensive resocialization into the new, deviant beliefs and behaviors; the demonization of people’s precult lives; intense punishment and shaming regimes; restrictions on exogenous social contacts; heavy financial and time commitments; and constant demands to value group commitments over family considerations. Certainly various factors, such as age, gender, access to wealth, prox- imity to the group’s central location, and so on, impact the extent to which family life is affected. Nevertheless, recent events such as the Heaven’s Gate suicides in 1997 indicate how deeply people can be involved with dangerous, collec- tivist thinking, which one scholar called “an ethic of radical obedience” (Davis, 2000, p. 249).4

One issue with social media is
Echo chambers are a possible harm of Graffiti,
however, what

Graffiti is able to reproduce echo chambers.
But will they exist in the dynamics?

currently, the siloing of centralized platforms and
even some decentralized systems ("defederation" in the fediverse)
means that a person curious about a fringe network (e.g. Gab, 8chan, incels.is)
needs to go "alone" and their activity will not "leak" to where their friends or
family can see it an intervene. Most “successful” cults follow a similar playbook
of cutting people off from their friends and family. A system with blurrier boundaries
between spaces - even if some of those spaces are “bad” - might help mitigate the formation
of cult-like structures, like filter bubbles.

\section{Evolution}

Currently, the design of social applications is dictated by a small handful of organization,
who are each themselves steered by a small handful of wealthy investors.
What social media sites would people want to use but are not profitable?
Either because there is not a ``critical mass'' of people interested (sub millions),
or the population in question is not one that could be profited on such as low-income or unhoused people.

What would a world look like when the evolution of apps is closer to user that.
As mentioned in~\cite{graffiti}, there are still not entirely accessible, but it is
more so and the path is reduced.

For example, will new patterns emerge.

\section{``Freedom of Speech'' versus ``Freedom of Reach''}

In Graffiti, speech is free but.

Tarleton, moderation:


- What benefits can Graffiti enable at scale? Empowering "one-size-fits-one" design? Evolution guided by user needs rather than platform needs? Normalizing non-hierarchical power? Replacing paternalistic policing with community accountability/care?



- What about the possible harms? "Classic web" concerns: CSAM, piracy, scams? Filter bubbles? Dogpiling (e.g. linking to / quoting someone's post without their permission)? Vandalism / platform co-option (e.g. replying to someone else's post without their permission)?
- What usage patterns will emerge? Will everyone use a different personalized app for each community they are in? Or will people use personalized "meta" apps that aggregate from all their communities? Will people opt for community moderation? Or prefer automated or third-party moderation services?
- Economics: Who will pay for the infrastructure? Who will pay the influencers? What other money flows exist in the system, e.g. aggregation services? Can Graffiti be "enshittified"?
- Law: Is Graffiti legal? (Is the web legal?) Could Graffiti be legally required? See GDPR or this utah bill
- Adoption: What does the adoption roadmap look like? Could it "bridge" to existing services?
- "Freedom of speech" vs "Freedom of reach"
- Social norms around scraping/crawling "unlisted" content (channels)
- Including survey/hackathon/student project data.
- Graffiti breaks the ability for people to add friction interventions. Pro social algorithms.
